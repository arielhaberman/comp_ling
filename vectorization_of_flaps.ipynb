{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Vectorization of IPA Symbols Applied to Flapping in English\n",
    "###### Ariel Haberman"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Corpus used was the Carnegie Mellon University Dictionary\n",
    "\n",
    "Credit/Thanks are given to Dr Danis and Zach Glabman\n",
    "\n",
    "Please excuse any typos/weird formatting because I wrote this all in markdown\n",
    "\n",
    "### Goals\n",
    "* See how well a classification model functions when given different amounts and types of data in the vectorization process\n",
    "* Personal Goal: actually learn how to vectorize textual data instead of relying on prebuilt methods for NLP/computation linguistics word\n",
    "\n",
    "### Steps\n",
    "* Create a working corpus of words by pulling data from CMU dictionary\n",
    "* Shorten each word to four letters for easier vectorization down the line\n",
    "* Vectorize the data\n",
    "* Fit a model with the data\n",
    "* Examine and theorize about why some types of vectorization work better than others\n",
    "* Try to figure out what could have been done better\n",
    "\n",
    "### Introduction\n",
    "The basic idea for this project comes from having read *Rumelhart & McClelland 1985 - On Learning the Past Tenses of English Verbs* and not liking some of the decisions they made but being incredibly interested in the process. Here, I have decided to focus on my grievances with their method of vectorization, and to explore other methods. To simplify the project I have changed which process the vectorization is tested on. I wanted to focus on the smallest level of vectorization and of language which made looking at flapping in English a more apt test case. This change is a purely phonological one, not a morphological or morphophonological one. By looking only at a phonological change, there are methods of vectorization that can be tried that otherwise would have been useless. It is also important to note, that since I worked on the phonological level stress had to be taken into account, something which I believe was ignored in the original paper.\n",
    "\n",
    "I had three main greivances with the Rumelhart and McClelland paper that I wanted to address here\n",
    "1. Instead of using the regular system of features from phonology they came up with their own system called \"Wickelfeatures'.\n",
    "2. They were unable to translate their vectorized data, and therefore the output of the model back into English.\n",
    "3. They called their features \"Wickeleatures\"\n",
    "\n",
    "All three of these grievances were addressed in this project. The first by using Bruce Hayes' phonological features. The second by creating at least two methods of vectorization that can be translated back into their original IPA. And the third, by actually naming my variable somewhat decent variable names for the first time in my life.\n",
    "\n",
    "### Background and Set Up\n",
    "* A flap will be represented by a 'dx' both in my notes and when the words are still in ARPABET form. This is modeled after *Gildea & Jurafsky 1996 - Learning bias and phonological-rule induction*\n",
    "\n",
    "* In English flapping occurs after a stressed vowel an optional 'r' and before an unstressed vowel, or t->dx \\ ˈV r* __ V. This means to create our dataset, the input data must include the unflapped form of words that contain the afformentioned environment and all the words that have a 't' but lack the same environment. The output data has the 't' changed into a flap in all the places that it should, and the same unchanged 't's for words that lack that environment.\n",
    "\n",
    "* All the words used here were pulled from the CMU dictionary. The CMU Dictionary distinguishes between unstressed, primary stressed and secondary stressed vowels. I made the decision to treat all stressed vowels as stressed without distinguishing between their levels of stress. They are all given the symbol for primary stress. This was done to simplify the vecotrization process later on. Without this change not all IPA characters would have a unique set of features. This would lead to needing a more complex vectorization system or another feature to be added. However, our environment is a boolean one that checks if the vowel is stressed or unstressed and doesn't care about the level of stress. Since the environment treats the two characters (primary and sceondary stressed vowels) the same I decided that the simplest solution to my problem would be to treat them like the same character (like the invironment does).\n",
    "\n",
    "* When shortening the words to four letters, the shortened word will always have the 't' or 'dx' in the third postion of the word. For words that contain the environment an unstressed vowel will always come last and the stressed vowel will come first or second depending on the presence of an 'r' before the 't'. By keeping the 't' in the third position, the environemnt for flapping is always found even in the shorter version of the word. This also made checking the accuracy of the vectorization process simpler as I always knew which line to check for the change done by flapping.\n",
    "\n",
    "* The features used in this project are an augmented version of Bruce Hayes' features. These were taken from the features spreadsheet found on the Ling 313 page (I am unsure if Dr Danis put together the excel page or if its from elsewhere). The spreadsheet was augmented by adding dipthongs following the guidelines set by Gildea & Jurafsky (1996). Dipthongs features match those of their first vowel and have an added feature of either w or y offglide. Hayes failed to account for a r-colored vowel (or I just couldn't find it) and this was added using the features found in the Pheatures app English inventory. The rhotasized vowel added was 'ɚ' as Wikipedia page on the ARPABET translates 'ER' as 'ɝ' but the pheatures spreadsheet only accounted for 'ɚ'. All vowels (including dipthongs) were then copied and a stressed version of the vowel was added to the dataframe. The stressed version contains a stress marker adjoining the IPA symbol and is +stress in the features instead of minus.\n",
    "\n",
    "* The features were brought into this project from the spreadsheet to a dataframe that will be refrenced below. Some of the augmentations were made to the features spreadsheet using excel, others were made using pandas. I will be uploading my edited version of the features spreadsheet with my submission, if you have any problems loading it please let me know.\n",
    "\n",
    "* Sonority as referenced in this project is also based on Bruce Hayes' features. The level of sonority of a sound is between 1 and 5 and categorized by the features of the sound. The features used to decide sonarity are +-syllabic, +-consonantal, +-approximant, and +-sonorant.\n",
    "\n",
    "* I added/created a version of sonority that differentiates between stressed and unstressed vowels. The version of sonority that includes stress breaks up the category of syllabic into stressed syllabic and unstressed syllabic. My understanding of sonority made me place stressed vowels as more sonorant than unstressed vowels. This is necessary because our environment differentiates between stressed and unstressed vowels and the model should be able to see that.\n",
    "\n",
    "### Types of Vectorization Examined\n",
    ">I vectorized the same set of data in six different ways, and for each used the same model and same training and testing data. The goal was to keep everything the same except for the method of vectorization. \n",
    "\n",
    "1. One Hot IPA Encoding - in this method of vectorization each sound or IPA symbol is represented by a vector the length of the number of IPA symbols. The vector contains a singular \"1\" a the index that the symbol appears in the features dataframe. This type of vectorization gives the model no background knowledge. The model does not know what a vowel is and how it might be different from a consonant. This is the most basic type of vectorization done in this project, where basic means 'lacking any sense of a universal grammer or inherent biases'.\n",
    "\n",
    "2. IPA Feature Vectorization - in this method, each symbol is represented (vectorized) as a list of their phonological features. All sounds are either + or - a particular feature which here is changed '1' or '0'. The vector is the length of the amount of features and is guaranteed to be unique for each sound. The model here is offered much more data about the sound itself and how it might be connected to another sound. Given to the model are the features used to seperate sounds into natural classes and create rules.\n",
    "\n",
    "3. One Hot Sonority Encoding - in this method of vectorization each sound is represented by a vector of length 5. Like with the one hot IPA encoding the vector contains a single 1 and the rest are zeros. The position of the 1 on the vector is determined by the sonority of the sound, where the first spot on the vector is the most sonorant and the last spot the least. In this version of vectorization the model is given some background knowledge.\n",
    "\n",
    "4. Sonority Feature Vectorization - in this method of vectorization each sound is represented by a shortened list of phonological features, only the ones used to determine the sonority level of a sound. Here, we are giving the model less data than we would be by giving it all the features, but also less noise to deal with as many features have been deemed unimportant and therefore cut.\n",
    "\n",
    ">It's important to realize that neither sonority vectorization model could truly learn the pattern. Although they scored better, sonority doesn't differentiate between stressed and unstressed vowels. This led me to create a version of sonority that differentiates between stressed and unstressed vowels.\n",
    "\n",
    "5. Stressed One Hot Sonority Encoding - same as above but with the changed made to the definition of sonority as mentioned in the background section.\n",
    "\n",
    "6. Stressed Sonority Feature Vectorization - same as above but with the changed made to the definition of sonority as mentioned in the background section. This does give the model more information than the previous version."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Comments should be read, they explain code and offer other insight. There are occasional larger blocks of texts that also offer explanations**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating a Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Importing CMU dict, splitting things up and subbing the ts for dxs\n",
    "import re\n",
    "import nltk\n",
    "import urllib.request\n",
    "cmu = nltk.corpus.cmudict.entries()\n",
    "\n",
    "#flapping happens when t comes between a stressed vowel followed by r* and an unstressed vowel\n",
    "joinedWords = []\n",
    "originalWords = []\n",
    "flappedWords = []\n",
    "otherWords = []\n",
    "\n",
    "#join the segments to make strings\n",
    "for word, pron in cmu:\n",
    "    joinedWords.append(\" \".join(pron))\n",
    "\n",
    "#make two word lists, one with the unflapped words and words that lack the envorinment\n",
    "#the other is flapped words and the words that lack the environment\n",
    "#lookbehind and look ahead can onyl take searches with a set number of characters so the version\n",
    "#of flapping with and without the r must be seperated into two seperate searches\n",
    "for w in joinedWords:\n",
    "    if(re.search(r\"(1|2) (R )*T ..0\", w)):\n",
    "        originalWords.append(w)\n",
    "        if(re.search(r\"(1|2) T ..0\", w)):\n",
    "            new = re.sub(r\"(?<=((1|2) ))T(?!..0)\",'dx', w)\n",
    "            flappedWords.append(new)\n",
    "        elif(re.search(r\"(1|2) R* T ..0\", w)):\n",
    "            newR = re.sub(r\"(?<=((1|2) R ))T(?!..0)\",'dx', w)\n",
    "            flappedWords.append(newR)\n",
    "    elif(re.search(r\" T \", w)):\n",
    "        otherWords.append(w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "#split the strings back into lists and add it to a list of lists\n",
    "#figure out where in the word the t occurs and keep track of how many spaces come before it\n",
    "# I feel like there should be a more clever way of doing this but I dont know what it was\n",
    "original = []\n",
    "ogSpaces = []\n",
    "for s in originalWords:\n",
    "    original.append(s.split())\n",
    "    if(re.search(r\"(1|2) T ..0\", s)):\n",
    "        t = re.search(r\"(?<=((1|2) ))T(?!..0)\", s)\n",
    "    else:\n",
    "        t = re.search(r\"(?<=((1|2) R ))T(?!..0)\", s)\n",
    "    span = list(t.span())\n",
    "    ind = span[1]\n",
    "    ogSpaces.append(s.count(\" \", 0, ind))\n",
    "    spaces = s.count(\" \", 0, ind)\n",
    "\n",
    "#list of the two segements before t and one after\n",
    "ogFour = []\n",
    "i = 0\n",
    "for lst in original:\n",
    "    count = ogSpaces[i]\n",
    "    four = lst[count-2:count+2]\n",
    "    ogFour.append(four)\n",
    "    i+=1\n",
    "\n",
    "#same as above just for dx\n",
    "flapped = []\n",
    "flapSpaces = []\n",
    "for s in flappedWords:\n",
    "    flapped.append(s.split())\n",
    "    if(re.search(r\"(1|2) dx ..0\", s)):\n",
    "        t = re.search(r\"(?<=((1|2) ))dx(?!..0)\", s)\n",
    "    else:\n",
    "        t = re.search(r\"(?<=((1|2) R ))dx(?!..0)\", s)\n",
    "    span = list(t.span())\n",
    "    ind = span[1]\n",
    "    flapSpaces.append(s.count(\" \", 0, ind))\n",
    "    spaces = s.count(\" \", 0, ind)\n",
    "\n",
    "#same as above just for dx\n",
    "flapFour = []\n",
    "j = 0\n",
    "for lst in flapped:\n",
    "    count = flapSpaces[j]\n",
    "    four = lst[count-2:count+2]\n",
    "    flapFour.append(four)\n",
    "    j+=1\n",
    "    \n",
    "#same as above just for words without the environment (other words)\n",
    "other = []\n",
    "otherSpaces = []\n",
    "for s in otherWords:\n",
    "    other.append(s.split())\n",
    "    if(re.search(r\"(?<=(.... ))T(...)\", s)):\n",
    "        t = re.search(r\"(?<=( ))T( )\", s)\n",
    "    span = list(t.span())\n",
    "    ind = span[1]\n",
    "    otherSpaces.append(s.count(\" \", 0, ind))\n",
    "    spaces = s.count(\" \", 0, ind)\n",
    "\n",
    "#same as above just for other words\n",
    "otherFour = []\n",
    "j = 0\n",
    "for lst in other:\n",
    "    count = otherSpaces[j]\n",
    "    four = lst[count-3:count+1]\n",
    "    otherFour.append(four)\n",
    "    j+=1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Modified from Dr Danis's code\n",
    "def getIPA(entry):\n",
    "    arpa_dict = {'AY' : 'aɪ',\n",
    "'D' : 'd',\n",
    "'IY' : 'i',\n",
    "'V' : 'v',\n",
    "'AE' : 'æ',\n",
    "'JH' : 'd͡ʒ',\n",
    "'UH' : 'ʊ',\n",
    "'T' : 't',\n",
    "'Y' : 'j',\n",
    "'AH' : 'ʌ',\n",
    "'G' : 'ɡ',\n",
    "'Z' : 'z',\n",
    "'P' : 'p',\n",
    "'TH' : 'θ',\n",
    "'M' : 'm',\n",
    "'R' : 'ɹ',\n",
    "'K' : 'k',\n",
    "'EH' : 'ɛ',\n",
    "'EY' : 'eɪ',\n",
    "'NG' : 'ŋ',\n",
    "'ZH' : 'ʒ',\n",
    "'HH' : 'h',\n",
    "'SH' : 'ʃ',\n",
    "'OY' : 'ɔɪ',\n",
    "'S' : 's',\n",
    "'AO' : 'ɔ',\n",
    "'F' : 'f',\n",
    "'W' : 'w',\n",
    "'IH' : 'ɪ',\n",
    "'DH' : 'ð',\n",
    "'L' : 'l',\n",
    "'N' : 'n',\n",
    "'CH' : 't͡ʃ',\n",
    "'AA' : 'ɑ',\n",
    "'B' : 'b',\n",
    "'OW' : 'oʊ',\n",
    "'UW' : 'u',\n",
    "'AW' : 'aʊ',\n",
    "'ER' : 'ɚ',\n",
    "'dx' : 'ɾ',\n",
    "'dxH' : 'θ',\n",
    "            ' '  :  ' '}\n",
    "    ipaList = []\n",
    "    for let in entry:\n",
    "        ipa = ''\n",
    "        if re.search(r\"0$\",let):\n",
    "            ipa = ipa + arpa_dict[re.sub(r\"\\d\",\"\",let)]\n",
    "\n",
    "        elif re.search(r\"(1$|2$)\",let):\n",
    "            ipa = ipa + 'ˈ' + arpa_dict[re.sub(r\"\\d\",\"\",let)]\n",
    "\n",
    "        else:\n",
    "            ipa = ipa + arpa_dict[re.sub(r\"\\d\",\"\",let)]\n",
    "\n",
    "        ipaList.append((ipa))\n",
    "    return ipaList"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#would be useful for going back and checking individual words against what the model outputted \n",
    "#decided against doing so mostly do to the confusion of making a test word in the first place\n",
    "#also making a test word would rely on all of my other work working and who knows if it\n",
    "#actually does\n",
    "ogIPA = []\n",
    "for word in original:\n",
    "    ogIPA.append(getIPA(word))\n",
    "\n",
    "flapIPA = []\n",
    "for word in flapped:\n",
    "    flapIPA.append(getIPA(word))\n",
    "\n",
    "#converting the lists of four letters (words) into IPA symbols\n",
    "ogFourIPA = []\n",
    "for word in ogFour:\n",
    "    ogFourIPA.append(getIPA(word))\n",
    "\n",
    "flapFourIPA = []\n",
    "for word in flapFour:\n",
    "    flapFourIPA.append(getIPA(word))  \n",
    "    \n",
    "otherIPA = []\n",
    "for word in other:\n",
    "    otherIPA.append(getIPA(word))\n",
    "\n",
    "otherFourIPA = []\n",
    "for word in otherFour:\n",
    "    otherFourIPA.append(getIPA(word))  \n",
    "    \n",
    "ogFourIPA.extend(otherFourIPA)\n",
    "flapFourIPA.extend(otherFourIPA)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hayes Phonological Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ariel\\anaconda3\\lib\\site-packages\\pandas\\core\\ops\\array_ops.py:253: FutureWarning: elementwise comparison failed; returning scalar instead, but in the future will perform elementwise comparison\n",
      "  res_values = method(rvalues)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "#import Hayes features\n",
    "df = pd.read_csv('hayes-features.csv', header=[0])\n",
    "\n",
    "#add in dipthong columns\n",
    "df = df.rename(columns={\"Unnamed: 0\": \"symbols\"})\n",
    "df['y offglide'] = 0\n",
    "df['w offglide'] = 0\n",
    "\n",
    "#change all values from +,-,0 to 0 for - and 1 for +\n",
    "df = df.where(df != '+', 1)\n",
    "df = df.where(df != '-', 0)\n",
    "\n",
    "#add dipthong glide things\n",
    "ai = df.index[df['symbols'] == 'aɪ'].tolist()\n",
    "df.at[ai, 'y offglide'] = 1\n",
    "ei = df.index[df['symbols'] == 'eɪ'].tolist()\n",
    "df.at[ei, 'y offglide'] = 1\n",
    "ui = df.index[df['symbols'] == 'ɔɪ'].tolist()\n",
    "df.at[ui, 'y offglide'] = 1\n",
    "au = df.index[df['symbols'] == 'aʊ'].tolist()\n",
    "df.at[au, 'w offglide'] = 1\n",
    "ou = df.index[df['symbols'] == 'oʊ'].tolist()\n",
    "df.at[ou, 'w offglide'] = 1\n",
    "\n",
    "#add stressed vowels\n",
    "vowels = df.where(df['syllabic']>0)\n",
    "vowels = vowels.dropna()\n",
    "vowels['stress'] = 1\n",
    "\n",
    "vowelsOne = vowels.copy()\n",
    "vowelsOne['symbols'] = 'ˈ' + vowelsOne['symbols']\n",
    "\n",
    "df = df.append(vowelsOne, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "#edited to add changes suggested by Dr Danis\n",
    "#make output binary classification instead of a vector\n",
    "\n",
    "#its a df of inputs and outputs where the words are tuples\n",
    "#tuples b/c they're immutable and want to delete deuplicates later\n",
    "flap_df = pd.DataFrame({'input': [tuple(x) for x in ogFourIPA], 'output': [tuple(x) for x in flapFourIPA]})\n",
    "\n",
    "#add a column to track changes\n",
    "#this should ideally be done at time of creating the output\n",
    "#but it's easier to do it this way for now\n",
    "flap_df['flapped'] = flap_df['input'] != flap_df['output']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>input</th>\n",
       "      <th>output</th>\n",
       "      <th>flapped</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>(b, ˈeɪ, t, ɪ)</td>\n",
       "      <td>(b, ˈeɪ, ɾ, ɪ)</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>(i, ˈeɪ, t, ʌ)</td>\n",
       "      <td>(i, ˈeɪ, ɾ, ʌ)</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>(i, ˈeɪ, t, ɪ)</td>\n",
       "      <td>(i, ˈeɪ, ɾ, ɪ)</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>(k, ˈeɪ, t, ʌ)</td>\n",
       "      <td>(k, ˈeɪ, ɾ, ʌ)</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>(k, ˈeɪ, t, ɪ)</td>\n",
       "      <td>(k, ˈeɪ, ɾ, ɪ)</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31661</th>\n",
       "      <td>(z, ˈɪ, ɡ, ɚ)</td>\n",
       "      <td>(z, ˈɪ, ɡ, ɚ)</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31664</th>\n",
       "      <td>(z, ˈaɪ, ʌ, n)</td>\n",
       "      <td>(z, ˈaɪ, ʌ, n)</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31665</th>\n",
       "      <td>(l, ˈɑ, t, ˈʌ)</td>\n",
       "      <td>(l, ˈɑ, t, ˈʌ)</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31669</th>\n",
       "      <td>(z, oʊ, ˈɑ, l)</td>\n",
       "      <td>(z, oʊ, ˈɑ, l)</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31670</th>\n",
       "      <td>(z, ˈu, t, s)</td>\n",
       "      <td>(z, ˈu, t, s)</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5867 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                input          output  flapped\n",
       "0      (b, ˈeɪ, t, ɪ)  (b, ˈeɪ, ɾ, ɪ)     True\n",
       "2      (i, ˈeɪ, t, ʌ)  (i, ˈeɪ, ɾ, ʌ)     True\n",
       "3      (i, ˈeɪ, t, ɪ)  (i, ˈeɪ, ɾ, ɪ)     True\n",
       "5      (k, ˈeɪ, t, ʌ)  (k, ˈeɪ, ɾ, ʌ)     True\n",
       "6      (k, ˈeɪ, t, ɪ)  (k, ˈeɪ, ɾ, ɪ)     True\n",
       "...               ...             ...      ...\n",
       "31661   (z, ˈɪ, ɡ, ɚ)   (z, ˈɪ, ɡ, ɚ)    False\n",
       "31664  (z, ˈaɪ, ʌ, n)  (z, ˈaɪ, ʌ, n)    False\n",
       "31665  (l, ˈɑ, t, ˈʌ)  (l, ˈɑ, t, ˈʌ)    False\n",
       "31669  (z, oʊ, ˈɑ, l)  (z, oʊ, ˈɑ, l)    False\n",
       "31670   (z, ˈu, t, s)   (z, ˈu, t, s)    False\n",
       "\n",
       "[5867 rows x 3 columns]"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#remove all unique values from input column to deal with words that become the same when reduced\n",
    "#to four segements\n",
    "#dont want same data point in train and test sets\n",
    "unique_df = flap_df.loc[~flap_df.duplicated(subset=['input'])]\n",
    "unique_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5867,)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([1, 1, 1, ..., 0, 0, 0])"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#y data / model output\n",
    "y = unique_df['flapped'].astype(int).to_numpy()\n",
    "print(y.shape)\n",
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "#CELLS MUST BE RUN IN ORDER\n",
    "#this cell overwrites previous data\n",
    "#every for-loop that does actual vectorization needs to iterate through the column of unique_df\n",
    "ogFourIPA = [list(word) for word in unique_df['input'].tolist()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# One Hot IPA Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['b', 'ˈeɪ', 't', 'ɪ'], ['i', 'ˈeɪ', 't', 'ʌ'], ['i', 'ˈeɪ', 't', 'ɪ'], ['k', 'ˈeɪ', 't', 'ʌ'], ['k', 'ˈeɪ', 't', 'ɪ']]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0.]])"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#This is the vectorization for the one hot encoding vectorization\n",
    "ogFourVec = []\n",
    "for word in ogFourIPA:\n",
    "    ogFourVec.append(word.copy())\n",
    "    \n",
    "print(ogFourIPA[:5])\n",
    "#https://stackoverflow.com/questions/21800169/python-pandas-get-index-of-rows-which-column-matches-certain-value\n",
    "#basically takes the column of symbols and makes it a 1 at that symbol and a 0 elsewhere\n",
    "i=0\n",
    "for word in ogFourVec:\n",
    "    j=0\n",
    "    fourVec = np.zeros((4, len(df)))\n",
    "    for letter in word: \n",
    "        ind = df.index[df['symbols'] == letter].tolist()\n",
    "        fourVec[j][ind] = 1\n",
    "        j+=1\n",
    "    ogFourVec[i] = fourVec\n",
    "    i+=1\n",
    "    \n",
    "ogFourVec[3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "(5867, 716)\n"
     ]
    }
   ],
   "source": [
    "#this was very much done with Dr Danis's help\n",
    "#Turning the 4 letter vectors into a single array per word\n",
    "i = 0\n",
    "for word in ogFourVec:\n",
    "    ogFourVec[i] = word.ravel(order='C')\n",
    "    i+=1\n",
    "print(ogFourVec[3])\n",
    "\n",
    "#turning the word arrays into a giant array of all the words\n",
    "ogFourArray = np.zeros((len(ogFourIPA), 4*len(df)))\n",
    "for index, word in enumerate(ogFourVec):\n",
    "    ogFourArray[index,:] = word\n",
    "    \n",
    "print(ogFourArray.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['b', 'ˈɪ', 't', 's'], ['ɑ', 'ɹ', 'k', 'ɪ'], ['p', 'ˈʌ', 't', 's'], ['s', 'ˈɑ', 't', 'h'], ['h', 'ɪ', 't', 'ʌ']]\n",
      "[0 0 0 0 0]\n",
      "[[0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 1. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]]\n",
      "[0 0 0 0 0]\n"
     ]
    }
   ],
   "source": [
    "#split the data set\n",
    "from sklearn.model_selection import train_test_split as tts\n",
    "\n",
    "#split the IPA data and not just vectorized for easier checking of correct vectorization\n",
    "#random state = 10 for all split so will always have the same words in the same groups/orders\n",
    "trainInIPA, testInIPA, trainOutIPA, testOutIPA = tts(ogFourIPA,y, test_size=.2, random_state=10)\n",
    "print(trainInIPA[:5])\n",
    "print(trainOutIPA[:5])\n",
    "\n",
    "trainIn, testIn, trainOut, testOut = tts(ogFourArray,y, test_size=.2, random_state=10)\n",
    "print(trainIn[:5])\n",
    "print(trainOut[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "score: 0.8867120954003407\n",
      "n: 1\n",
      "score: 0.8986371379897785\n",
      "n: 2\n",
      "score: 0.919931856899489\n",
      "n: 3\n",
      "score: 0.9258943781942078\n",
      "n: 4\n",
      "score: 0.9361158432708688\n",
      "n: 5\n",
      "score: 0.9437819420783645\n",
      "n: 6\n",
      "score: 0.9437819420783645\n",
      "n: 7\n",
      "score: 0.9471890971039182\n",
      "n: 8\n",
      "score: 0.9565587734241908\n",
      "n: 9\n",
      "score: 0.9531516183986372\n",
      "n: 10\n",
      "score: 0.9625212947189097\n",
      "n: 11\n",
      "score: 0.9599659284497445\n",
      "n: 12\n",
      "score: 0.9633730834752982\n",
      "n: 13\n",
      "score: 0.9642248722316865\n",
      "n: 14\n",
      "winner: 0.9642248722316865\n",
      "winner index: 14\n"
     ]
    }
   ],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "# warning this will take a bit of time\n",
    "# this model actually takes the longest\n",
    "# the best fit is when k=14 so you can just run that if you prefer (commented out below)\n",
    "oldScore = 0\n",
    "ind=0\n",
    "for i in range(1,15):\n",
    "    new_model = KNeighborsClassifier(n_neighbors=i)\n",
    "    new_model.fit(trainIn,trainOut)\n",
    "    score = new_model.score(testIn, testOut)\n",
    "    print('score: ' + str(score))\n",
    "    print('n: ' + str(i))\n",
    "    if(score>oldScore):\n",
    "        oldScore=score\n",
    "        ind = i\n",
    "        \n",
    "oneHotIPAWinner = oldScore\n",
    "print('winner: ' + str(oldScore))\n",
    "print('winner index: ' + str(ind))\n",
    "\n",
    "# new_model = KNeighborsClassifier(n_neighbors=14)\n",
    "# new_model.fit(trainIn,trainOut)\n",
    "# score = new_model.score(testIn, testOut)\n",
    "# oneHotIPAWinner = score\n",
    "# print(score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Vector Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['b', 'ˈeɪ', 't', 'ɪ'], ['i', 'ˈeɪ', 't', 'ʌ'], ['i', 'ˈeɪ', 't', 'ɪ'], ['k', 'ˈeɪ', 't', 'ʌ'], ['k', 'ˈeɪ', 't', 'ɪ']]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[1., 0., 0., 0., 1., 1., 0., 1., 0., 0., 0., 1., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 1., 1., 0., 1., 0., 1., 0., 0.],\n",
       "       [1., 1., 0., 0., 1., 1., 0., 1., 0., 0., 0., 1., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 1., 0., 0., 1., 0., 1., 1., 0.],\n",
       "       [0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [1., 0., 0., 0., 1., 1., 0., 1., 0., 0., 0., 1., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 1., 0., 0., 0.]])"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(ogFourIPA[:5])\n",
    "ogFourFeatVec = []\n",
    "for word in ogFourIPA:\n",
    "    ogFourFeatVec.append(word.copy())\n",
    "\n",
    "#this basically takes a row from the dataframe instead of taking a column\n",
    "i=0\n",
    "for word in ogFourFeatVec:\n",
    "    j=0\n",
    "    fourVec = np.zeros((4, df.shape[1]-1))\n",
    "    for letter in word:\n",
    "        ind = df.index[df['symbols'] == letter].tolist()\n",
    "        fourVec[j] = df.iloc[ind, 1:].values[0]\n",
    "        j+=1\n",
    "    ogFourFeatVec[i] = fourVec\n",
    "    i+=1\n",
    "    \n",
    "ogFourFeatVec[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1. 0. 0. 0. 1. 1. 0. 1. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1.\n",
      " 0. 1. 0. 1. 0. 0. 1. 1. 0. 0. 1. 1. 0. 1. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 1. 0. 0. 1. 0. 1. 1. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 1. 1.\n",
      " 0. 1. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 1. 0. 0. 0.]\n"
     ]
    }
   ],
   "source": [
    "#same as above (also you helped me with this stuff)\n",
    "#Turning the 4 letter vectors into a single array per word\n",
    "i = 0\n",
    "for word in ogFourFeatVec:\n",
    "    ogFourFeatVec[i] = word.ravel(order='C')\n",
    "    i+=1\n",
    "print(ogFourFeatVec[1])\n",
    "\n",
    "#turning the word arrays into a giant array of all the words\n",
    "ogFourFeatArray = np.zeros((len(ogFourIPA), 4*(df.shape[1]-1)))\n",
    "for index, word in enumerate(ogFourFeatVec):\n",
    "    ogFourFeatArray[index,:] = word\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 1., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [1., 1., 0., 0., 1., 1., 0., 1., 0., 0., 0., 1., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 1., 0., 0., 1., 0., 1., 1., 0.],\n",
       "       [0., 0., 0., 1., 1., 1., 0., 1., 1., 0., 0., 1., 0., 0., 0., 0.,\n",
       "        0., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [1., 0., 0., 0., 1., 1., 0., 1., 0., 0., 0., 1., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 1., 1., 0., 1., 0., 0., 0., 0.]])"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "flapFourFeatVec = []\n",
    "for word in flapFourIPA:\n",
    "    flapFourFeatVec.append(word.copy())\n",
    "\n",
    "i=0\n",
    "for word in flapFourFeatVec:\n",
    "    j=0\n",
    "    fourVec = np.zeros((4, df.shape[1]-1))\n",
    "    for letter in word:\n",
    "        ind = df.index[df['symbols'] == letter].tolist()\n",
    "        fourVec[j] = df.iloc[ind, 1:].values[0]\n",
    "        j+=1\n",
    "    flapFourFeatVec[i] = fourVec\n",
    "    i+=1\n",
    "    \n",
    "flapFourFeatVec[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1. 0. 0. 0. 1. 1. 0. 1. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1.\n",
      " 0. 1. 0. 1. 0. 0. 1. 1. 0. 0. 1. 1. 0. 1. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 1. 0. 0. 1. 0. 1. 1. 0. 0. 0. 0. 1. 1. 1. 0. 1. 1. 0. 0. 1.\n",
      " 0. 0. 0. 0. 0. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 1. 1.\n",
      " 0. 1. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 0. 1. 0. 0. 0. 0.]\n"
     ]
    }
   ],
   "source": [
    "#Turning the 4 letter vectors into a single array per word\n",
    "i = 0\n",
    "for word in flapFourFeatVec:\n",
    "    flapFourFeatVec[i] = word.ravel(order='C')\n",
    "    i+=1\n",
    "print(flapFourFeatVec[3])\n",
    "\n",
    "#turning the word arrays into a giant array of all the words\n",
    "flapFourFeatArray = np.zeros((len(flapFourIPA), 4*(df.shape[1]-1)))\n",
    "for index, word in enumerate(flapFourFeatVec):\n",
    "    flapFourFeatArray[index,:] = word\n",
    "    \n",
    "flapFourFeatArrayFlat = flapFourFeatArray.flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n: 1\n",
      "score: 0.9156729131175468\n",
      "n: 2\n",
      "score: 0.9190800681431005\n",
      "n: 3\n",
      "score: 0.9395229982964225\n",
      "n: 4\n",
      "score: 0.944633730834753\n",
      "n: 5\n",
      "score: 0.9403747870528109\n",
      "n: 6\n",
      "score: 0.948892674616695\n",
      "n: 7\n",
      "score: 0.9429301533219762\n",
      "n: 8\n",
      "score: 0.9531516183986372\n",
      "n: 9\n",
      "score: 0.9497444633730835\n",
      "n: 10\n",
      "score: 0.9531516183986372\n",
      "n: 11\n",
      "score: 0.9471890971039182\n",
      "n: 12\n",
      "score: 0.9531516183986372\n",
      "n: 13\n",
      "score: 0.9471890971039182\n",
      "n: 14\n",
      "score: 0.9522998296422487\n",
      "winner index: 8\n",
      "winner: 0.9531516183986372\n"
     ]
    }
   ],
   "source": [
    "trainInFeat, testInFeat, trainOutFeat, testOutFeat = tts(ogFourFeatArray,y, test_size=.2, random_state=10)\n",
    "\n",
    "oldScore = 0\n",
    "ind=0\n",
    "for i in range(1,15):\n",
    "    new_model = KNeighborsClassifier(n_neighbors=i)\n",
    "    new_model.fit(trainInFeat,trainOutFeat)\n",
    "    score = new_model.score(testInFeat, testOutFeat)\n",
    "    print('n: ' + str(i))\n",
    "    print('score: ' + str(score))\n",
    "    if(score>oldScore):\n",
    "        oldScore=score\n",
    "        ind = i\n",
    "\n",
    "featuresIPAWinner = oldScore\n",
    "print('winner index: ' + str(ind))\n",
    "print('winner: ' + str(oldScore))\n",
    "\n",
    "#Just runs the best k\n",
    "# new_model = KNeighborsClassifier(n_neighbors=8)\n",
    "# new_model.fit(trainInFeat,trainOutFeat)\n",
    "# score = new_model.score(testInFeat, testOutFeat)\n",
    "# featuresIPAWinner = score\n",
    "# print(score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sonarity Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Its all just hard coded\n",
    "v = 0\n",
    "g = 1\n",
    "l = 2\n",
    "n = 3\n",
    "o = 4\n",
    "\n",
    "#I thought about pulling this data from the features dataframe each time and realized that this \n",
    "# was much much easier\n",
    "vow = np.array([1, 0, 1, 1])\n",
    "gld = np.array([0, 0, 1, 1])\n",
    "liq = np.array([0, 1, 1, 1])\n",
    "nas = np.array([0, 1, 0, 1])\n",
    "obs = np.array([0, 1, 0, 0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# One Hot Sonarity Encoding\n",
    "## This was somehow the most succesful model \n",
    "##### Which is only a tiny bit suspicious"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['l', 'ˈu', 't', 'ɪ']\n",
      "[[0. 0. 1. 0. 0.]\n",
      " [1. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 1.]\n",
      " [1. 0. 0. 0. 0.]]\n"
     ]
    }
   ],
   "source": [
    "ogFourSonVec = []\n",
    "for word in ogFourIPA:\n",
    "    ogFourSonVec.append(word.copy())\n",
    "\n",
    "#it works like the IPA one hot encoding but it checks the features of a given symbol to figure out\n",
    "#what the sonarity score is\n",
    "i=0\n",
    "for word in ogFourSonVec:\n",
    "    j=0\n",
    "    fourVec = np.zeros((4, 5))\n",
    "    for letter in word:\n",
    "        ind = df.index[df['symbols'] == letter].tolist()\n",
    "        #vowels\n",
    "        if(df.loc[ind[0]].at['syllabic'] > 0):\n",
    "            fourVec[j][v] = 1\n",
    "        #glides\n",
    "        elif(df.loc[ind[0]].at['consonantal'] == 0):\n",
    "            fourVec[j][g] = 1\n",
    "        #obstruents\n",
    "        elif (df.loc[ind[0]].at['sonorant'] == 0):\n",
    "            fourVec[j][o] = 1\n",
    "        #nasals\n",
    "        elif(df.loc[ind[0]].at['approximant'] == 0):\n",
    "            fourVec[j][n] = 1\n",
    "        #liquids\n",
    "        else:\n",
    "            fourVec[j][l] = 1\n",
    "        j+=1\n",
    "    ogFourSonVec[i] = fourVec\n",
    "    i+=1\n",
    "    \n",
    "print(ogFourIPA[10])  \n",
    "print(ogFourSonVec[10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0. 0. 1. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 0. 0. 0. 0.]\n"
     ]
    }
   ],
   "source": [
    "#Turning the 4 letter vectors into a single array per word\n",
    "i = 0\n",
    "for word in ogFourSonVec:\n",
    "    ogFourSonVec[i] = word.ravel(order='C')\n",
    "    i+=1\n",
    "print(ogFourSonVec[10])\n",
    "\n",
    "#turning the word arrays into a giant array of all the words\n",
    "ogFourSonArray = np.zeros((len(ogFourIPA), 4*5))\n",
    "for index, word in enumerate(ogFourSonVec):\n",
    "    ogFourSonArray[index,:] = word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n: 1\n",
      "score: 0.8219761499148212\n",
      "n: 2\n",
      "score: 0.82793867120954\n",
      "n: 3\n",
      "score: 0.8160136286201022\n",
      "n: 4\n",
      "score: 0.82793867120954\n",
      "n: 5\n",
      "score: 0.823679727427598\n",
      "n: 6\n",
      "score: 0.82793867120954\n",
      "n: 7\n",
      "score: 0.82793867120954\n",
      "n: 8\n",
      "score: 0.82793867120954\n",
      "n: 9\n",
      "score: 0.823679727427598\n",
      "n: 10\n",
      "score: 0.82793867120954\n",
      "n: 11\n",
      "score: 0.82793867120954\n",
      "n: 12\n",
      "score: 0.82793867120954\n",
      "n: 13\n",
      "score: 0.82793867120954\n",
      "n: 14\n",
      "score: 0.82793867120954\n",
      "winner index: 2\n",
      "winner: 0.82793867120954\n"
     ]
    }
   ],
   "source": [
    "trainInSon, testInSon, trainOutSon, testOutSon = tts(ogFourSonArray,y, test_size=.2, random_state=10)\n",
    "\n",
    "#this is where things get really sketchy\n",
    "#how are there perfect scores all around? \n",
    "oldScore = 0\n",
    "ind=0\n",
    "for i in range(1,15):\n",
    "    new_model = KNeighborsClassifier(n_neighbors=i)\n",
    "    new_model.fit(trainInSon,trainOutSon)\n",
    "    score = new_model.score(testInSon, testOutSon)\n",
    "    print('n: ' + str(i))\n",
    "    print('score: ' + str(score))\n",
    "    if(score>oldScore):\n",
    "        oldScore=score\n",
    "        ind = i\n",
    "\n",
    "oneHotSonWinner = oldScore\n",
    "print('winner index: ' + str(ind))\n",
    "print('winner: ' + str(oldScore))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sonarity Feature Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['l', 'ˈu', 't', 'ɪ']\n",
      "[[0. 1. 1. 1.]\n",
      " [1. 0. 1. 1.]\n",
      " [0. 1. 0. 0.]\n",
      " [1. 0. 1. 1.]]\n"
     ]
    }
   ],
   "source": [
    "ogFourSonFeatVec = []\n",
    "for word in ogFourIPA:\n",
    "    ogFourSonFeatVec.append(word.copy())\n",
    "\n",
    "#instead of taking slices of the row from the main dataframe like above, we use the hardcoded \n",
    "#arrays for the vectorization\n",
    "i=0\n",
    "for word in ogFourSonFeatVec:\n",
    "    j=0\n",
    "    fourVec = np.zeros((4, 4))\n",
    "    for letter in word:\n",
    "        ind = df.index[df['symbols'] == letter].tolist()\n",
    "        #vowels\n",
    "        if(df.loc[ind[0]].at['syllabic'] == 1):\n",
    "            fourVec[j] = vow\n",
    "        #glides\n",
    "        elif(df.loc[ind[0]].at['consonantal'] == 0):\n",
    "            fourVec[j] = gld\n",
    "        #obstruents\n",
    "        elif (df.loc[ind[0]].at['sonorant'] == 0):\n",
    "            fourVec[j] = obs\n",
    "        #nasals\n",
    "        elif(df.loc[ind[0]].at['approximant'] == 0):\n",
    "            fourVec[j] = nas\n",
    "        #liquids\n",
    "        else:\n",
    "            fourVec[j] = liq\n",
    "        j+=1\n",
    "    ogFourSonFeatVec[i] = fourVec\n",
    "    i+=1\n",
    "    \n",
    "print(ogFourIPA[10])  \n",
    "print(ogFourSonFeatVec[10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Turning the 4 letter vectors into a single array per word\n",
    "i = 0\n",
    "for word in ogFourSonFeatVec:\n",
    "    ogFourSonFeatVec[i] = word.ravel(order='C')\n",
    "    i+=1\n",
    "\n",
    "#turning the word arrays into a giant array of all the words\n",
    "ogFourSonFeatArray = np.zeros((len(ogFourIPA), 4*4))\n",
    "for index, word in enumerate(ogFourSonFeatVec):\n",
    "    ogFourSonFeatArray[index,:] = word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n: 1\n",
      "score: 0.8160136286201022\n",
      "n: 2\n",
      "score: 0.82793867120954\n",
      "n: 3\n",
      "score: 0.7921635434412265\n",
      "n: 4\n",
      "score: 0.8015332197614992\n",
      "n: 5\n",
      "score: 0.8015332197614992\n",
      "n: 6\n",
      "score: 0.82793867120954\n",
      "n: 7\n",
      "score: 0.8015332197614992\n",
      "n: 8\n",
      "score: 0.8015332197614992\n",
      "n: 9\n",
      "score: 0.7947189097103918\n",
      "n: 10\n",
      "score: 0.8015332197614992\n",
      "n: 11\n",
      "score: 0.7947189097103918\n",
      "n: 12\n",
      "score: 0.8015332197614992\n",
      "n: 13\n",
      "score: 0.8015332197614992\n",
      "n: 14\n",
      "score: 0.8015332197614992\n",
      "winner index: 2\n",
      "winner: 0.82793867120954\n"
     ]
    }
   ],
   "source": [
    "trainInSonFeat, testInSonFeat, trainOutSonFeat, testOutSonFeat = tts(ogFourSonFeatArray,y, test_size=.2, random_state=10)\n",
    "\n",
    "oldScore = 0\n",
    "ind=0\n",
    "for i in range(1,15):\n",
    "    new_model = KNeighborsClassifier(n_neighbors=i)\n",
    "    new_model.fit(trainInSonFeat,trainOutSonFeat)\n",
    "    score = new_model.score(testInSonFeat, testOutSonFeat)\n",
    "    print('n: ' + str(i))\n",
    "    print('score: ' + str(score))\n",
    "    if(score>oldScore):\n",
    "        oldScore=score\n",
    "        ind = i\n",
    "\n",
    "featSonWinner = oldScore\n",
    "print('winner index: ' + str(ind))\n",
    "print('winner: ' + str(oldScore))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sonarity with Stress"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Its all just hard coded\n",
    "#syllabic is being replaced by two categories - syllabic stressed and syllabic unstressed\n",
    "vs = 0 #stressed\n",
    "vu = 1 #unstressed\n",
    "g = 2\n",
    "l = 3\n",
    "n = 4\n",
    "o = 5\n",
    "\n",
    "# here the columns are syllabic, stress, consonantal, approximant, sonorant\n",
    "vows = np.array([1, 1, 0, 1, 1])\n",
    "vowu = np.array([1, 0, 0, 1, 1])\n",
    "gld = np.array([0, 0, 0, 1, 1])\n",
    "liq = np.array([0, 0, 1, 1, 1])\n",
    "nas = np.array([0, 0, 1, 0, 1])\n",
    "obs = np.array([0, 0, 1, 0, 0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## One Hot Sonority Encoding with Stress\n",
    "#### Which still produces a suspicious model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['l', 'ˈu', 't', 'ɪ']\n",
      "[[0. 0. 0. 1. 0. 0.]\n",
      " [1. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 1.]\n",
      " [0. 1. 0. 0. 0. 0.]]\n",
      "[0. 0. 0. 1. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 1. 0. 0. 0. 0.]\n"
     ]
    }
   ],
   "source": [
    "ogFourSonStressVec = []\n",
    "for word in ogFourIPA:\n",
    "    ogFourSonStressVec.append(word.copy())\n",
    "\n",
    "#it works like the IPA one hot encoding but it checks the features of a given symbol to figure out\n",
    "#what the sonarity score is\n",
    "i=0\n",
    "for word in ogFourSonStressVec:\n",
    "    j=0\n",
    "    fourVec = np.zeros((4, 6))\n",
    "    for letter in word:\n",
    "        ind = df.index[df['symbols'] == letter].tolist()\n",
    "        #vowels\n",
    "        if(df.loc[ind[0]].at['syllabic'] > 0):\n",
    "            if(df.loc[ind[0]].at['stress'] > 0):\n",
    "                fourVec[j][vs] = 1\n",
    "            else:\n",
    "                fourVec[j][vu] = 1\n",
    "        #glides\n",
    "        elif(df.loc[ind[0]].at['consonantal'] == 0):\n",
    "            fourVec[j][g] = 1\n",
    "        #obstruents\n",
    "        elif (df.loc[ind[0]].at['sonorant'] == 0):\n",
    "            fourVec[j][o] = 1\n",
    "        #nasals\n",
    "        elif(df.loc[ind[0]].at['approximant'] == 0):\n",
    "            fourVec[j][n] = 1\n",
    "        #liquids\n",
    "        else:\n",
    "            fourVec[j][l] = 1\n",
    "        j+=1\n",
    "    ogFourSonStressVec[i] = fourVec\n",
    "    i+=1\n",
    "    \n",
    "print(ogFourIPA[10])  \n",
    "print(ogFourSonStressVec[10])\n",
    "\n",
    "#Turning the 4 letter vectors into a single array per word\n",
    "i = 0\n",
    "for word in ogFourSonStressVec:\n",
    "    ogFourSonStressVec[i] = word.ravel(order='C')\n",
    "    i+=1\n",
    "print(ogFourSonStressVec[10])\n",
    "\n",
    "#turning the word arrays into a giant array of all the words\n",
    "ogFourSonStressArray = np.zeros((len(ogFourIPA), 4*6))\n",
    "for index, word in enumerate(ogFourSonStressVec):\n",
    "    ogFourSonStressArray[index,:] = word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n: 1\n",
      "score: 0.9480408858603067\n",
      "n: 2\n",
      "score: 0.9540034071550255\n",
      "n: 3\n",
      "score: 0.9778534923339012\n",
      "n: 4\n",
      "score: 0.9540034071550255\n",
      "n: 5\n",
      "score: 0.9514480408858603\n",
      "n: 6\n",
      "score: 0.9514480408858603\n",
      "n: 7\n",
      "score: 0.975298126064736\n",
      "n: 8\n",
      "score: 0.975298126064736\n",
      "n: 9\n",
      "score: 0.975298126064736\n",
      "n: 10\n",
      "score: 0.975298126064736\n",
      "n: 11\n",
      "score: 0.975298126064736\n",
      "n: 12\n",
      "score: 0.975298126064736\n",
      "n: 13\n",
      "score: 0.975298126064736\n",
      "n: 14\n",
      "score: 0.975298126064736\n",
      "winner index: 3\n",
      "winner: 0.9778534923339012\n"
     ]
    }
   ],
   "source": [
    "trainInSonStress, testInSonStress, trainOutSonStress, testOutSonStress= tts(ogFourSonStressArray,y, test_size=.2, random_state=10)\n",
    "\n",
    "oldScore = 0\n",
    "ind=0\n",
    "for i in range(1,15):\n",
    "    new_model = KNeighborsClassifier(n_neighbors=i)\n",
    "    new_model.fit(trainInSonStress,trainOutSonStress)\n",
    "    score = new_model.score(testInSonStress, testOutSonStress)\n",
    "    print('n: ' + str(i))\n",
    "    print('score: ' + str(score))\n",
    "    if(score>oldScore):\n",
    "        oldScore=score\n",
    "        ind = i\n",
    "\n",
    "oneHotSonStressWinner = oldScore\n",
    "print('winner index: ' + str(ind))\n",
    "print('winner: ' + str(oldScore))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stressed Sonority Feature Vectorization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['l', 'ˈu', 't', 'ɪ']\n",
      "[[0. 0. 1. 1. 1.]\n",
      " [1. 1. 0. 1. 1.]\n",
      " [0. 0. 1. 0. 0.]\n",
      " [1. 0. 0. 1. 1.]]\n"
     ]
    }
   ],
   "source": [
    "ogFourSonFeatStressVec = []\n",
    "for word in ogFourIPA:\n",
    "    ogFourSonFeatStressVec.append(word.copy())\n",
    "\n",
    "#instead of taking slices of the row from the main dataframe like above, we use the hardcoded \n",
    "#arrays for the vectorization\n",
    "i=0\n",
    "for word in ogFourSonFeatStressVec:\n",
    "    j=0\n",
    "    fourVec = np.zeros((4, 5))\n",
    "    for letter in word:\n",
    "        ind = df.index[df['symbols'] == letter].tolist()\n",
    "        #vowels\n",
    "        if(df.loc[ind[0]].at['syllabic'] == 1):\n",
    "            if(df.loc[ind[0]].at['stress'] > 0):\n",
    "                fourVec[j] = vows\n",
    "            else:\n",
    "                fourVec[j] = vowu\n",
    "        #glides\n",
    "        elif(df.loc[ind[0]].at['consonantal'] == 0):\n",
    "            fourVec[j] = gld\n",
    "        #obstruents\n",
    "        elif (df.loc[ind[0]].at['sonorant'] == 0):\n",
    "            fourVec[j] = obs\n",
    "        #nasals\n",
    "        elif(df.loc[ind[0]].at['approximant'] == 0):\n",
    "            fourVec[j] = nas\n",
    "        #liquids\n",
    "        else:\n",
    "            fourVec[j] = liq\n",
    "        j+=1\n",
    "    ogFourSonFeatStressVec[i] = fourVec\n",
    "    i+=1\n",
    "    \n",
    "print(ogFourIPA[10])  \n",
    "print(ogFourSonFeatStressVec[10])\n",
    "\n",
    "#Turning the 4 letter vectors into a single array per word\n",
    "i = 0\n",
    "for word in ogFourSonFeatStressVec:\n",
    "    ogFourSonFeatStressVec[i] = word.ravel(order='C')\n",
    "    i+=1\n",
    "\n",
    "#turning the word arrays into a giant array of all the words\n",
    "ogFourSonFeatStressArray = np.zeros((len(ogFourIPA), 4*5))\n",
    "for index, word in enumerate(ogFourSonFeatStressVec):\n",
    "    ogFourSonFeatStressArray[index,:] = word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n: 1\n",
      "score: 0.9727427597955707\n",
      "n: 2\n",
      "score: 0.944633730834753\n",
      "n: 3\n",
      "score: 0.9787052810902896\n",
      "n: 4\n",
      "score: 0.9684838160136287\n",
      "n: 5\n",
      "score: 0.9787052810902896\n",
      "n: 6\n",
      "score: 0.9787052810902896\n",
      "n: 7\n",
      "score: 0.9761499148211243\n",
      "n: 8\n",
      "score: 0.9761499148211243\n",
      "n: 9\n",
      "score: 0.9761499148211243\n",
      "n: 10\n",
      "score: 0.9761499148211243\n",
      "n: 11\n",
      "score: 0.9761499148211243\n",
      "n: 12\n",
      "score: 0.9761499148211243\n",
      "n: 13\n",
      "score: 0.9744463373083475\n",
      "n: 14\n",
      "score: 0.9744463373083475\n",
      "winner index: 3\n",
      "winner: 0.9787052810902896\n"
     ]
    }
   ],
   "source": [
    "trainInSonFeatStress, testInSonFeatStress, trainOutSonFeatStress, testOutSonFeatStress = tts(ogFourSonFeatStressArray,y, test_size=.2, random_state=10)\n",
    "\n",
    "oldScore = 0\n",
    "ind=0\n",
    "for i in range(1,15):\n",
    "    new_model = KNeighborsClassifier(n_neighbors=i)\n",
    "    new_model.fit(trainInSonFeatStress,trainOutSonFeatStress)\n",
    "    score = new_model.score(testInSonFeatStress, testOutSonFeatStress)\n",
    "    print('n: ' + str(i))\n",
    "    print('score: ' + str(score))\n",
    "    if(score>oldScore):\n",
    "        oldScore=score\n",
    "        ind = i\n",
    "\n",
    "featSonStressWinner = oldScore\n",
    "print('winner index: ' + str(ind))\n",
    "print('winner: ' + str(oldScore))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Final Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "One Hot IPA Encoding Vectorization: 96.42%\n",
      "Hayes Phonetics Features Vectorization: 95.32%.\n",
      "\n",
      "One Hot Sonarity Encoding Vectorization: 82.79%\n",
      "Sonarity Phonetics Features Vectorization: 82.79%\n",
      "\n",
      "Stressed One Hot Sonarity Encoding Vectorization: 97.79%\n",
      "Stressed Sonarity Phonetics Features Vectorization: 97.87%\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"One Hot IPA Encoding Vectorization: \" + str(round(oneHotIPAWinner*100, 2)) + '%')\n",
    "print(\"Hayes Phonetics Features Vectorization: {:0.2f}%.\\n\".format(featuresIPAWinner*100))\n",
    "print(\"One Hot Sonarity Encoding Vectorization: \" + str(round(oneHotSonWinner*100, 2)) + '%')\n",
    "print(\"Sonarity Phonetics Features Vectorization: {:0.2f}%\\n\".format(featSonWinner*100))\n",
    "print(\"Stressed One Hot Sonarity Encoding Vectorization: \" + str(round(oneHotSonStressWinner*100, 2)) + '%')\n",
    "print(\"Stressed Sonarity Phonetics Features Vectorization: {:0.2f}%\\n\".format(featSonStressWinner*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">Coming into this project I thought for sure the phonological feature vectorization of the model would perform better than the one hot IPA encoding. As much as this was a very simple test case I am very suprised at how well the one hot IPA encoding version did. The accuracy was pretty much the same for both models that just used phonological features. However the accuracy of the model given a k that is not the top k for the feature model was typically higher than the accuracy given from the one hot encoding model with the same k.\n",
    "\n",
    ">As seen by the model scores, sonority was the better way to vecorize the text for this given task. However, I'm still really suspicious of the perfectly performing sonority one hot encodings. As much as I looked over my code I keep feeling like I must have done somthing wrong to get those perfect scores. I did find it interesting that the sonority features vectorization was more accurate when stress was added. \n",
    "\n",
    ">The downside to vectorizing the data using sonority scores is you run into a similar problem to what Rumelhart and McClellan had. You can undo the flattening of the various arrays the just use IPA symbols and features. You can then work backwards using the index of the one in the vector or just checking features to figure out what your original inputs were. That can't be done using the more accurate sonority models. The higher accuracy models are actually less useful.\n",
    "\n",
    ">This leads to an interesting discussion of which method was actually better. The method that produced a more accurate model or the method that has usable outputs?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mistakes and Where to go From Here\n",
    ">There's a problem with how the training/test split was originally done. By only preserving 5 segments of the word, there has been large amounts of neutralization, in that certain words with the same flapping environment now have identical data points. So when the training/test split happens, you essentially end up with the same data point in the test data as in the train data, making the accuracy artifically high. This might be fixed now but its hard to know.\n",
    "\n",
    ">I tried using some other models, namely SGDClassifier and Logistic Regression, however both involved my data having a very different shape. When I vectorized my data I used arrays and didnt label anything with gave me issues with both other types of models. I think most of these models are used in cases where y outputs a single data point, or something else that isnt a whole vector the size of the input data.\n",
    "\n",
    ">So although I wasnt that happy with KNN for either of the first two methods of vectorization I still stuck with it. I also wanted to use the same model across all the methods of vectorization because this was really about comparing the vectors not the models.\n",
    "\n",
    ">The takeaway from all this being that while I am very happy with the ideas behind the vectorization of the data I am missing a fundamental part which was properly preparing it for a model.\n",
    "\n",
    ">If I had used a different model I would have struggled with the process of reshaping my arrays and unvectorizing my data to see what the model outputed. This can only be done for the IPA/phonological methods of vectorization and not the sonority ones, but is still a theoretical step that can be done and I am not doing. This might actually require a prediction model though and not a classification model like the one I used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
